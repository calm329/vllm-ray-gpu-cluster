apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-head
  namespace: vllm-cluster
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm-head
  template:
    metadata:
      labels:
        app: vllm-head
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                - key: role
                  operator: In
                  values:
                    - master
      containers:
        - name: vllm-head
          image: calm329/vllm-mistral
          ports:
            - containerPort: 6379  # Ray head node port
            - containerPort: 8000  # vLLM serving port
          resources:
            limits:
              nvidia.com/gpu: 1  # Adjust based on available GPUs on the master node
          volumeMounts:
            - mountPath: /root/.cache/huggingface
              name: huggingface-cache
          command: ["/bin/sh", "-c", "ray start --head --port=6379 --block & vllm serve /app/mistral-7b-instruct-v0.2-code-ft.Q4_K_S.gguf --port 8000 --max-num-batched-tokens 24000 --max-model-len 512 --max-num-seqs 40 --host 0.0.0.0 --tensor-parallel-size 1 --pipeline-parallel-size 2"]
      volumes:
        - name: huggingface-cache
          hostPath:
            path: /home/ubuntu/huggingface/cache
